{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329b7254-8713-4921-8432-bf89f2339a83",
   "metadata": {},
   "source": [
    "## Code for reproducing analyses from \"Zhang & Yu, 2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29622df9-355c-4849-85d3-0f7a51388069",
   "metadata": {},
   "source": [
    "### EEG experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313eaad-272d-4cff-a818-23876751b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne, glob, os, mne_rsa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score,KFold\n",
    "import pingouin as pg\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import rc\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.decomposition import PCA\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import font_manager\n",
    "from sklearn.utils import resample\n",
    "from mne import io\n",
    "from mne_connectivity import spectral_connectivity_epochs, seed_target_indices, spectral_connectivity_time\n",
    "from mne.time_frequency import AverageTFR, tfr_morlet\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mne.stats import spatio_temporal_cluster_test, combine_adjacency, spatio_temporal_cluster_1samp_test\n",
    "from mne.channels import find_ch_adjacency\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "plt.rcParams['font.family']= 'sans-serif'\n",
    "plt.rcParams['font.sans-serif']= 'Helvetica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6c892-ef70-4d84-bb87-8a6135c46846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "epochs=[]\n",
    "metadata=[]\n",
    "for i in range(22):\n",
    "    epochs.append(mne.read_epochs('s0%s_eeg_cleaned-epo.fif.gz'%(i+1)))\n",
    "    metadata.append(epochs[-1].metedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815fd72-7311-4a09-b177-d4b981aef761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define frontal middle and posterior channels\n",
    "picks_f=[a for a in reconst_epochs_p2.ch_names if 'F' in a and 'C' not in a and 'T' not in a]\n",
    "picks_p=[a for a in reconst_epochs_p2.ch_names if 'P' in a and  'T' not in a and 'C' not in a or 'O' in a]\n",
    "picks_m = [a for a in reconst_epochs_p2.ch_names if 'C' in a or 'T' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a2d43-abd8-4176-b64a-6adb19011e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_null(rdms, use_pca=True):\n",
    "    temp=[]\n",
    "    for ss, rdm_arr in enumerate(rdms):\n",
    "        circularity=[]\n",
    "        for i in range(len(w_start[:-1])): \n",
    "            scaler = StandardScaler()\n",
    "            embedding =  PCA(n_components=2)\n",
    "            if use_pca:\n",
    "                coor = pca_ind[ss][i].transform(scaler.fit_transform(rdm_arr[i]))\n",
    "            else:\n",
    "                coor = embedding.fit_transform(scaler.fit_transform(rdm_arr[i]))\n",
    "            c1x =coor[:,0]\n",
    "            c1y =coor[:,1]\n",
    "            polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[2],c1y[2]),(c1x[0],c1y[0])))\n",
    "            circularity.append(4*np.pi*polygon.area/(polygon.length)**2)\n",
    "        temp.append(circularity)      \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097fb47-2881-42b9-a3f2-4477895659cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_trial_label(picks):\n",
    "    resampled_rdm=[]\n",
    "    for t in range(5):\n",
    "        rdms_ss_v=[]   \n",
    "        for i, f in enumerate(epochs):\n",
    "            m = metadata[i]\n",
    "            m['trial_type'] = 0 #recode trials into trial_types based on task feature of interest\n",
    "            m['target_size'] = np.where(m.answer_size.values<0.19, 1,np.where(m.answer_size.values>0.26, 3, 2))\n",
    "            m['target_color'] = np.where(m.answer_color.values<42, 1,np.where(m.answer_color.values>74, 3, 2))\n",
    "            x_train = f.copy().pick(picks).get_data()#select channels\n",
    "            # for j,l in enumerate(list(product((1,2,3),(1,2,3)))): #for stimulus space\n",
    "            #     m.loc[(m['whichSize']==l[0])&(m['whichColor']==l[1]), 'trial_type'] = j\n",
    "            # for j,l in enumerate(list(product((1,2,3),(1,2,3)))): #for target stimulus space\n",
    "            #     m.loc[(m['target_size']==l[0])&(m['target_color']==l[1]), 'trial_type'] = j\n",
    "            y_train =  m.cue.values -1 #for goal space\n",
    "            # y_train = m['trial_type'].values # for stimulus or target spaces\n",
    "            if t!=0:\n",
    "                x_train, y_train= resample(x_train, y_train, stratify=y_train)\n",
    "            n_trials, n_sensors, n_times= x_train.shape \n",
    "            eeg_data =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))\n",
    "            np.random.shuffle(y_train) #shuffle trial labels\n",
    "            for i in range(np.unique(y_train).shape[0]):\n",
    "                eeg_data[i,:,:] = np.mean(x_train[y_train==i],axis=0)\n",
    "            w_start = np.where(np.isin(f.times, f.times[0::20])==True)[0]\n",
    "            rdms = []\n",
    "            for i in range(len(w_start[:-1])): \n",
    "                rdms.append(np.mean(eeg_data[:, :, w_start[i]:w_start[i+1]], axis=2))\n",
    "            rdms_ss_v.append(rdms)\n",
    "        resampled_rdm.append(rdms_ss_v)\n",
    "    return resampled_rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edac80-56e5-4b30-b143-5591e9ed1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_statistic():\n",
    "    ci_null = np.array(shuffled_ci).mean(1)[np.random.choice(np.arange(np.array(shuffled_ci).mean(1).shape[0]))]\n",
    "    pvalues_null=np.zeros(len(w_start[:-1]))\n",
    "    for j in range(pvalues.shape[0]):\n",
    "        pvalues_null[j] = np.where(np.array(shuffled_ci).mean(1)[:,j]>ci_null[j])[0].shape[0]/len(np.array(shuffled_ci).mean(1))\n",
    "\n",
    "    d = np.where(pvalues_null<0.05)[0] #get clusters bigger than 0\n",
    "    a = np.split(d,np.where(np.diff(d)!=1)[0]+1) #get consecutive points\n",
    "    return np.max([len(x) for x in a]) #count location only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11caad4a-9ce4-4bfd-84a8-a17fe6143964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_label_splittrial(picks,stimulus=False, exclude_trial=False):\n",
    "    scaler = MinMaxScaler() #0-1 range\n",
    "    rdms_ss_good, rdms_ss_bad=[],[]   \n",
    "    for i, f in enumerate(epochs):\n",
    "        x_train = f.copy().pick(picks).get_data()#select channels\n",
    "        \n",
    "        m = metadata[i]#recode trials into trial_types based on all features\n",
    "        m['trial_type'] = 0\n",
    "        m['response'] = 0\n",
    "        m['error_color'] =  (m.answer_color-m.correct_color) \n",
    "        m['error_size'] = (m.answer_size-m.correct_size)\n",
    "        m['error_size_per'] = m.error_size/m['size'] \n",
    "        m['error_color'] =  scaler.fit_transform(m.error_color.values.reshape(-1,1))\n",
    "        m['error_size_per'] =  scaler.fit_transform(m.error_size_per.values.reshape(-1,1))\n",
    "        total_error =m['error_color'].values+m['error_size_per'].values\n",
    "        for j,l in enumerate(list(product((1,2,3),(1,2,3)))): #test stimulus space\n",
    "            m.loc[(m['whichSize']==l[0])&(m['whichColor']==l[1]), 'trial_type'] = j\n",
    "\n",
    "        #using absolute error values\n",
    "        if stimulus==True :\n",
    "            ind_goodtrial = m.loc[(np.percentile(total_error, 50)> total_error) ].index.values #half-half\n",
    "            ind_badtrial = m.loc[(np.percentile(total_error,50)<= total_error) ].index.values\n",
    "        else:\n",
    "            ind_goodtrial = m.loc[(np.percentile(total_error, 25)> total_error) ].index.values #upper and lower quantile\n",
    "            ind_badtrial = m.loc[(np.percentile(total_error,75)<= total_error) ].index.values\n",
    "            if i==10: #one subject doesnt have enough trials so loosen the threshold a bit\n",
    "                ind_goodtrial = m.loc[(np.percentile(total_error, 25)> total_error) ].index.values\n",
    "                ind_badtrial = m.loc[(np.percentile(total_error, 70)<= total_error) ].index.values\n",
    "\n",
    "        y_train = m.cue.values-1 #for goal space\n",
    "        if stimulus==True: #for stimulus space\n",
    "            y_train = m['trial_type'].values\n",
    "\n",
    "        #split trials based on error magnitude\n",
    "        xtrain_badtrial = x_train[ind_badtrial]\n",
    "        xtrain_goodtrial = x_train[ind_goodtrial]\n",
    "        ytrain_badtrial = y_train[ind_badtrial]\n",
    "        ytrain_goodtrial = y_train[ind_goodtrial]\n",
    "        \n",
    "        #randomly assign membership between the good and bad trials\n",
    "        x1,x2, y1,y2 =train_test_split(np.concatenate([xtrain_badtrial,xtrain_goodtrial]), np.concatenate([ytrain_badtrial,ytrain_goodtrial]), \\\n",
    "                                       test_size=0.5, train_size=0.5, stratify=np.concatenate([ytrain_badtrial,ytrain_goodtrial]))\n",
    "        n_trials, n_sensors, n_times= x_train.data.shape \n",
    "        eeg_data_badtrial =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))\n",
    "        eeg_data_goodtrial =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))\n",
    "        for i in range(np.unique(y_train).shape[0]): #average within conditions\n",
    "            eeg_data_badtrial[i,:,:] = np.mean(x1[y1==i],axis=0)\n",
    "            eeg_data_goodtrial[i,:,:] = np.mean(x2[y2==i],axis=0)\n",
    "        w_start = np.where(np.isin(f.times, f.times[0::20])==True)[0]\n",
    "        rdms_bad,rdms_good = [],[]\n",
    "        for n in range(len(w_start[:-1])): \n",
    "            rdms_bad.append(np.mean(eeg_data_badtrial[:, :, w_start[n]:w_start[n+1]], axis=2))\n",
    "            rdms_good.append(np.mean(eeg_data_goodtrial[:, :, w_start[n]:w_start[n+1]], axis=2))\n",
    "        rdms_ss_good.append(rdms_good)\n",
    "        rdms_ss_bad.append(rdms_bad)\n",
    "    return rdms_ss_good, rdms_ss_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee56fc1-2e6f-4b9a-94b5-6e209227b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_label_splittrial(picks, stimulus=False):\n",
    "    scaler = MinMaxScaler() #0-1 range\n",
    "    rdms_ss_good, rdms_ss_bad=[],[]   \n",
    "    for i, f in enumerate(epochs):\n",
    "        x_train = f.copy().pick(picks).get_data()#select channels\n",
    "        #recode trials into trial_types based on all features\n",
    "        m = metadata[i]\n",
    "        m['trial_type'] = 0\n",
    "        m['response'] = 0\n",
    "\n",
    "        #exclude large error trials\n",
    "        m['error_color'] =  (m.answer_color-m.correct_color) \n",
    "        m['error_size'] = (m.answer_size-m.correct_size)\n",
    "        m['error_size_per'] = m.error_size/m['size'] \n",
    "        m['error_color'] =  scaler.fit_transform(m.error_color.values.reshape(-1,1))\n",
    "        m['error_size_per'] =  scaler.fit_transform(m.error_size_per.values.reshape(-1,1))\n",
    "        total_error =m['error_color'].values+m['error_size_per'].values\n",
    "        for j,l in enumerate(list(product((1,2,3),(1,2,3)))): #test stimulus space\n",
    "            m.loc[(m['whichSize']==l[0])&(m['whichColor']==l[1]), 'trial_type'] = j\n",
    "        m['target_size'] = np.where(m.answer_size.values<0.19, 1,np.where(m.answer_size.values>0.26, 3, 2))\n",
    "        m['target_color'] = np.where(m.answer_color.values<42, 1,np.where(m.answer_color.values>74, 3, 2))\n",
    "        for j,l in enumerate(list(product((1,2,3),(1,2,3)))): #test stimulus space\n",
    "            m.loc[(m['target_size']==l[0])&(m['target_color']==l[1]), 'response'] = j\n",
    "        \n",
    "        #using absolute error values\n",
    "        if stimulus==True or stimulus=='response':\n",
    "            ind_goodtrial = m.loc[(np.percentile(total_error, 50)> total_error) ].index.values\n",
    "            ind_badtrial = m.loc[(np.percentile(total_error,50)<= total_error) ].index.values\n",
    "        else:\n",
    "            ind_goodtrial = m.loc[(np.percentile(total_error, 25)> total_error) ].index.values\n",
    "            ind_badtrial = m.loc[(np.percentile(total_error,75)<= total_error) ].index.values\n",
    "            if i==10:\n",
    "                ind_goodtrial = m.loc[(np.percentile(total_error, 25)> total_error) ].index.values\n",
    "                ind_badtrial = m.loc[(np.percentile(total_error, 70)<= total_error) ].index.values\n",
    "\n",
    "        y_train = m.cue.values-1\n",
    "        if stimulus==True:\n",
    "            y_train = m['trial_type'].values\n",
    "        elif stimulus=='response':\n",
    "            y_train = m['response'].values\n",
    "        n_trials, n_sensors, n_times= x_train.data.shape \n",
    "\n",
    "        #split half\n",
    "        xtrain_badtrial = x_train[ind_badtrial]\n",
    "        xtrain_goodtrial = x_train[ind_goodtrial]\n",
    "        ytrain_badtrial = y_train[ind_badtrial]\n",
    "        ytrain_goodtrial = y_train[ind_goodtrial]\n",
    "\n",
    "        #if we are not resampling just using real data\n",
    "        x_badtrial,y_badtrial=  xtrain_badtrial , ytrain_badtrial\n",
    "        x_goodtrial,y_goodtrial = xtrain_goodtrial, ytrain_goodtrial\n",
    "\n",
    "        eeg_data_badtrial =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))\n",
    "        eeg_data_goodtrial =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))\n",
    "        for i in range(np.unique(y_train).shape[0]):\n",
    "            eeg_data_badtrial[i,:,:] = np.mean(x_badtrial[y_badtrial==i],axis=0)\n",
    "            eeg_data_goodtrial[i,:,:] = np.mean(x_goodtrial[y_goodtrial==i],axis=0)\n",
    "        w_start = np.where(np.isin(f.times, f.times[0::20])==True)[0]#if want to draw time course\n",
    "        rdms_bad,rdms_good = [],[]\n",
    "        for n in range(len(w_start[:-1])): \n",
    "            rdms_bad.append(np.mean(eeg_data_badtrial[:, :, w_start[n]:w_start[n+1]], axis=2)) #not using similarity matrix just use raw data\n",
    "            rdms_good.append(np.mean(eeg_data_goodtrial[:, :, w_start[n]:w_start[n+1]], axis=2))\n",
    "        rdms_ss_good.append(rdms_good)\n",
    "        rdms_ss_bad.append(rdms_bad)\n",
    "    return rdms_ss_good, rdms_ss_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d607fc-f571-4a24-aa4b-048d806b86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_null(rdms, ss, pcas, scalers):\n",
    "    temp=[]\n",
    "    for i in range(len(w_start[:-1])):\n",
    "        scaler=StandardScaler()\n",
    "        pca= PCA(2)\n",
    "        coor = pcas[i].transform(scaler.fit_transform(rdms[i])) #using the original pca and scaler trained by entire dataset\n",
    "        # coor = pcas[i].transform(scaler.fit_transform(np.delete(rdms[i], [1,4,7],axis=0))) \n",
    "        c1x =coor[:,0]\n",
    "        c1y =coor[:,1]\n",
    "        polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[2],c1y[2]),(c1x[0],c1y[0])))\n",
    "        # polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[5],c1y[5]),(c1x[4],c1y[4]),(c1x[2],c1y[2]),(c1x[0],c1y[0])))\n",
    "        temp.append(4*np.pi*polygon.area/(polygon.length)**2)  \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d50da-db29-439b-9961-6d30bd621915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 1B\n",
    "#prepare data; We resample the raw data a few times to get rid of randomness\n",
    "resampled_rdms=[]\n",
    "for t in range(5):\n",
    "    rdms_ss_v=[]   \n",
    "    scaler=StandardScaler()\n",
    "    for i, f in enumerate(epochs):\n",
    "        m = metadata[i]\n",
    "        x_train = f.copy().pick(picks_f).get_data()#select channels\n",
    "        y_train =  m.cue.values -1  #for plotting goal circularity\n",
    "        n_trials, n_sensors, n_times= x_train.shape \n",
    "        if t!=0:\n",
    "            x_train, y_train= resample(x_train, y_train, stratify=y_train)\n",
    "        eeg_data =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))# eeg_data 1st dim matches num of features in conceptual model\n",
    "        for k in range(np.unique(y_train).shape[0]):\n",
    "            eeg_data[k,:,:] = np.mean(x_train[y_train==k],axis=0)\n",
    "        w_start = np.where(np.isin(f.times, f.times[0::20])==True)[0]#if want to draw time course, we use sliding window of 80 ms long\n",
    "        rdms = []\n",
    "        for n in range(len(w_start[:-1])): \n",
    "            rdms.append(np.mean(eeg_data[:, :, w_start[n]:w_start[n+1]], axis=2)) \n",
    "        rdms_ss_v.append(rdms)\n",
    "    resampled_rdms.append(rdms_ss_v)    \n",
    "    \n",
    "#calculate real individual circularity\n",
    "resampled_ci=[]\n",
    "for t in range(5):\n",
    "    temp=[]\n",
    "    if t==0:\n",
    "        pca_ind , scaler_ind= [],[]\n",
    "    for ss, rdm_arr in enumerate(resampled_rdms[t]):\n",
    "        m= metadata[ss]\n",
    "        circularity=[]\n",
    "        temp1,temp2=[],[]\n",
    "        for i in range(len(w_start[:-1])):\n",
    "            scaler = StandardScaler()\n",
    "            embedding =  PCA(n_components=2)\n",
    "            coor = embedding.fit_transform(scaler.fit_transform(rdm_arr[i]))\n",
    "            temp1.append(embedding);temp2.append(scaler)\n",
    "            c1x =coor[:,0]\n",
    "            c1y =coor[:,1]\n",
    "            polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[2],c1y[2]),(c1x[0],c1y[0])))\n",
    "            circularity.append(4*np.pi*polygon.area/(polygon.length)**2)\n",
    "        if t==0:\n",
    "            pca_ind.append(temp1);scaler_ind.append(temp2)\n",
    "        temp.append(circularity) \n",
    "    resampled_ci.append(temp)\n",
    "    \n",
    "individual_ci=[]\n",
    "for t in range(len(w_start[:-1])):\n",
    "    individual_ci.append(np.array(resampled_ci).mean(0)[:,t])\n",
    "individual_ci=np.array(individual_ci).T\n",
    "\n",
    "#get distribution of shuffled data matrices\n",
    "#Can take 5-8 hours depending on the CPU\n",
    "#Note that to create distribution of different circularity indices (goals vs. stimulus), you need to manually change the y_train in the function\n",
    "shuffled_rdm_ss= Parallel(n_jobs=25)(delayed(shuffle_trial_label)(picks_f) for i in range(5000))\n",
    "\n",
    "\n",
    "#calculating individual circularity using the shuffled data \n",
    "w_start = np.where(np.isin(f.times, f.times[0::20])==True)[0]#if want to draw time course\n",
    "shuffled_ci_resampled =[]\n",
    "for t in range(5):\n",
    "    #use Parallel to speed up\n",
    "    shuffled_ci = Parallel(n_jobs=25)(delayed(calculate_null)(rdms,use_pca=t==0) for rdms in np.array(shuffled_rdm_ss)[:,t])\n",
    "    shuffled_ci_resampled.append(shuffled_ci)\n",
    "shuffled_ci = np.array(shuffled_ci_resampled).mean(0) #if resampling trials first averaging over resmapled folds\n",
    "\n",
    "#manual cluster-level statistics\n",
    "cluster_perm_total=  Parallel(n_jobs=20)(delayed(cluster_statistic)() for i in range(5000))\n",
    "threshold_total = np.percentile(cluster_perm_total, 95, method='closest_observation')\n",
    "coor=[]\n",
    "for x in a:\n",
    "    if len(x)>threshold_total:\n",
    "        coor.extend(x) \n",
    "    \n",
    "#Plotting\n",
    "fig, ax =plt.subplots(1,1,figsize=(6,3.))\n",
    "data = individual_ci\n",
    "plt.plot(f.times[w_start[:-1]+20], data.mean(0), c='orange', label='difference')\n",
    "plt.fill_between(f.times[w_start[:-1]+20],data.mean(0)+ data.std(0)/np.sqrt(data.shape[0]),data.mean(0)- data.std(0)/np.sqrt(data.shape[0]),alpha=0.2,color='orange')\n",
    "plt.scatter(np.take(f.times[w_start[:-1]+20], coor), [0.25]*len(coor),c='r',marker='_')#highlight significant period\n",
    "ax.axvline(0,c='k')\n",
    "ax.axvspan(0.4,1.7, alpha=.15,color='gray')\n",
    "ax.axvspan(2.3,3.8, alpha=.15,color='gray')\n",
    "ax.set_ylabel('Individual circularity',fontsize=16)\n",
    "ax.set_xlabel('Time (s)',fontsize=15)\n",
    "ax.set_yticks([0.2,0.3,0.4])\n",
    "ax.set_yticklabels([0.2,0.3,0.4],fontsize=15, va='center')\n",
    "ax.set_xticks([0,0.4,1.7,2.3,3.8])\n",
    "ax.set_xticklabels(['0',0.4,1.7,2.3,3.8] ,fontsize=15, va='center')\n",
    "ax.tick_params(bottom=True)\n",
    "ax.xaxis.set_tick_params(pad=10)\n",
    "for y in [0.4,1.7,2.3,3.8]:\n",
    "    ax.axvline(y, linestyle='--', color='k', linewidth=0.8)\n",
    "sns.despine(fig=fig, right=True, top=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e330d58-e82d-45cd-967a-6e4380e5fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 1C\n",
    "#prepare data matrix\n",
    "rdms_ss_v=[]   \n",
    "for i, f in enumerate(epochs):\n",
    "    m = metadata[i]\n",
    "    x_train = f.copy().pick(picks_f).get_data()#select channels\n",
    "    y_train =  m.cue.values -1  #for plotting goal circularity\n",
    "    n_trials, n_sensors, n_times= x_train.shape \n",
    "    eeg_data =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))# eeg_data 1st dim matches num of features in conceptual model\n",
    "    for k in range(np.unique(y_train).shape[0]):\n",
    "        eeg_data[k,:,:] = np.mean(x_train[y_train==k],axis=0)#average within conditions\n",
    "    w_start = np.where(np.isin(f.times, [-0.5,0,0.4,1.7,2.3,3.8,4.3])==True)[0]  #select the time windows corresponding to task epochs\n",
    "    rdms = []\n",
    "    for n in range(len(w_start[:-1])): \n",
    "        rdms.append(np.mean(eeg_data[:, :, w_start[n]:w_start[n+1]], axis=2)) #average within time windows\n",
    "    rdms_ss_v.append(rdms)\n",
    "\n",
    "#perform PCA\n",
    "dfs_mds=[]\n",
    "categorical_model = [1,2,3,4]\n",
    "for i in range(len(w_start[:-1])):\n",
    "    scaler = StandardScaler()\n",
    "    a = [x[i] for x in rdms_ss_v] #concatenate subjects \n",
    "    embedding = PCA(n_components=3)\n",
    "    coor = embedding.fit_transform(scaler.fit_transform(np.hstack(a))) #standardize again before applying PCA\n",
    "    df = pd.DataFrame(coor, columns=['PC1','PC2','PC3'])\n",
    "    df['cue'] = categorical_model \n",
    "    df['time']=i\n",
    "    dfs_mds.append(df)\n",
    "dfs_mds = pd.concat(dfs_mds, ignore_index=True)\n",
    "    \n",
    "#Plotting\n",
    "palettes = ['yellow','green','r','darkblue']\n",
    "axs=axs.ravel()\n",
    "circularity_xy=[]\n",
    "titles=['Pre-cue','Goal cue','Delay 1','Sample','Delay 2','Response']\n",
    "dim1='PC1'; dim2='PC2'\n",
    "for i in range(len(w_start[:-1])):\n",
    "    data=dfs_mds[dfs_mds.time==i].groupby(['cue']).mean().reset_index()\n",
    "    c1x =data[dim1].values\n",
    "    c1y =data[dim2].values\n",
    "    axs[i].plot([c1x[0],c1x[1],c1x[3],c1x[2],c1x[0]],[c1y[0],c1y[1],c1y[3],c1y[2],c1y[0]],lw=1.4, c='gray', zorder=1)\n",
    "    sns.scatterplot(x=dim1, y=dim2, data=data, hue= 'cue',legend=False, ax=axs[i], palette=palettes, s=130,edgecolor='black',zorder=2, )\n",
    "    axs[i].set_title(titles[i], fontsize=22)\n",
    "    axs[i].set_xlabel('PC1',fontsize=16)\n",
    "    axs[i].set_ylabel('PC2',fontsize=16)\n",
    "    axs[i].set(xticklabels=[],yticklabels=[])\n",
    "    axs[i].tick_params(bottom=False, left=False)\n",
    "    polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[2],c1y[2]),(c1x[0],c1y[0])))\n",
    "    circularity_xy.append(4*np.pi*polygon.area/(polygon.length)**2) #calculate the circularity index\n",
    "\n",
    "sns.despine(fig=fig, right=True, top=True)\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1a260-0a03-4e10-ab24-2e4fc26c831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 2A\n",
    "#prepare data matrix\n",
    "rdms_ss_v=[]   \n",
    "for i, f in enumerate(epochs):\n",
    "    m = metadata[i]\n",
    "    x_train = f.copy().pick(picks_f).get_data()#select channels\n",
    "    y_train =  m.cue.values -1  #for plotting goal circularity\n",
    "    n_trials, n_sensors, n_times= x_train.shape \n",
    "    eeg_data =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))# eeg_data 1st dim matches num of features in conceptual model\n",
    "    for k in range(np.unique(y_train).shape[0]):\n",
    "        eeg_data[k,:,:] = np.mean(x_train[y_train==k],axis=0)#average within conditions\n",
    "    w_start = np.where(np.isin(f.times, [-0.5,0,0.4,1.7,2.3,3.8,4.3])==True)[0]  #select the time windows corresponding to task epochs\n",
    "    rdms = []\n",
    "    for n in range(len(w_start[:-1])): \n",
    "        rdms.append(np.mean(eeg_data[:, :, w_start[n]:w_start[n+1]], axis=2)) #average within time windows\n",
    "    rdms_ss_v.append(rdms)\n",
    "\n",
    "dfs_mds_ss =[]\n",
    "categorical_model = np.array([1,2,3,4]) \n",
    "for ss, rdm_arr in enumerate(rdms_ss_v):\n",
    "    m= metadata[ss]\n",
    "    m['error_color'] =  (m.answer_color-m.correct_color) #no within subject scaling, do it across subjects\n",
    "    m['error_size'] = (m.answer_size-m.correct_size)\n",
    "    m['error_size_per'] = m.error_size/m['size'] #size is a percentile construt\n",
    "    a =m.shape[0]\n",
    "    for i in range(len(w_start[:-1])):\n",
    "        scaler = StandardScaler()\n",
    "        embedding =  PCA(n_components=3)\n",
    "        coor = embedding.fit_transform(scaler.fit_transform(rdm_arr[i]))\n",
    "        df = pd.DataFrame(np.hstack((coor,categorical_model.reshape(-1,1))), columns=[0,1,2,'cue'])\n",
    "        df['time']=i\n",
    "        df['subject']=ss+1\n",
    "        df['error_color'] = m.error_color.abs().mean()\n",
    "        df['error_size'] = m.error_size.abs().mean()\n",
    "        df['error_size_per'] = m.error_size_per.abs().mean()\n",
    "        dfs_mds_ss.append(df)\n",
    "\n",
    "scaler = MinMaxScaler()      \n",
    "dfs_mds_ss = pd.concat(dfs_mds_ss, ignore_index=True) \n",
    "#scale the error terms across subjects before adding up for total error\n",
    "dfs_mds_ss['error_color'] =  scaler.fit_transform(dfs_mds_ss.error_color.values.reshape(-1,1))\n",
    "dfs_mds_ss['error_size'] =  scaler.fit_transform(dfs_mds_ss.error_size.values.reshape(-1,1))\n",
    "dfs_mds_ss['error_size_per'] =  scaler.fit_transform(dfs_mds_ss.error_size_per.values.reshape(-1,1))\n",
    "dfs_mds_ss['error_total'] = dfs_mds_ss['error_color']+dfs_mds_ss['error_size_per']\n",
    "\n",
    "for ss in range(len(rdms_ss_v)):\n",
    "    for i in range(len(w_start[:-1])):\n",
    "        for dims in list(itertools.combinations((0,1,2),2)):\n",
    "            dim1=dims[0]\n",
    "            dim2=dims[1]\n",
    "            c1x =dfs_mds_ss[(dfs_mds_ss.subject==ss+1)&(dfs_mds_ss.time==i)][dim1].values\n",
    "            c1y =dfs_mds_ss[(dfs_mds_ss.subject==ss+1)&(dfs_mds_ss.time==i)][dim2].values\n",
    "            polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[2],c1y[2]),(c1x[0],c1y[0])))\n",
    "            dfs_mds_ss.loc[(dfs_mds_ss.subject==ss+1)&(dfs_mds_ss.time==i), 'ci_%s%s'%(dim1,dim2)] = 4*np.pi*polygon.area/(polygon.length)**2\n",
    "\n",
    "#Plotting\n",
    "fig, ax=plt.subplots(1,1,figsize=(5.5,4))\n",
    "ax.tick_params(left=False, bottom=False)\n",
    "sns.regplot( dfs_mds_ss.loc[dfs_mds_ss.time==2].groupby(['subject'])['ci_%s%s'%(dims[0],dims[1])].mean(),\n",
    "               dfs_mds_ss.loc[dfs_mds_ss.time==2].groupby(['subject']).error_total.mean())\n",
    "ax.set_ylabel('Standardized response error', fontsize=16)\n",
    "ax.set_xlabel('Circularity index', fontsize=16)\n",
    "ax.set_xticks([0,0.2,0.4,0.6])\n",
    "ax.set_xticklabels([0,0.2,0.4,0.6],rotation=0 ,fontsize=16)\n",
    "ax.set_yticks([0,0.5,1,1.5])\n",
    "ax.set_yticklabels([0,0.5,1,1.5],rotation=0 ,fontsize=16)\n",
    "sns.despine(fig=fig,top=True, right=True)\n",
    "ax.text(0.05,1.5, r'$r = 0.17, p = 0.78$',fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b0957-6b59-4565-a436-a7a0e6f88ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 3B-D\n",
    "#get shuffled null distribution of good and bad trial \n",
    "#take ~3 hours\n",
    "temps= Parallel(n_jobs=30)(delayed(permute_label_splittrial)(picks_f,stimulus=False) for i in range(5000))#for stimulus space, set stimulus parameter to True\n",
    "shuffled_rdm_ss_good = [items[0] for items in temps]\n",
    "shuffled_rdm_ss_bad = [items[1] for items in temps]\n",
    "\n",
    "#calculate circularity difference \n",
    "trial_cond = ['good','bad']\n",
    "w_start = np.where(np.isin(f.times, f.times[0::20])==True)[0]#if want to draw time course\n",
    "dimensions=[]\n",
    "for k,rdms_ss in enumerate([shuffled_rdm_ss_good,shuffled_rdm_ss_bad]):\n",
    "    temp=[]\n",
    "    for ss in range(len(metadata)):\n",
    "        temp1 = Parallel(n_jobs=25)(delayed(run_null)(rdms_ss[n][ss], ss,  pca_ind[ss], scaler_ind[ss]) for n in range(len(rdms_ss)))\n",
    "        temp.append(temp1)\n",
    "    dimensions.append(np.mean(temp,0))\n",
    "data = np.array(dimensions[0])-np.array(dimensions[1])\n",
    "\n",
    "#calculate ciruclarity differnce for real data split\n",
    "rdms_ss_good, rdms_ss_bad = shuffle_label_splittrial(picks_f,stimulus=False)                \n",
    "dimensions=[]\n",
    "for k,rdms_ss in enumerate([rdms_ss_good, rdms_ss_bad]):\n",
    "    temp=[]\n",
    "    for ss in range(len(metadata)):\n",
    "        temp1=[]\n",
    "        for i in range(len(w_start[:-1])):\n",
    "            pca= PCA(2)\n",
    "            coor = pca_ind[ss][i].transform(scaler_ind[ss][i].transform(rdms_ss[ss][i])) #standardize again before applying PCA\n",
    "            c1x =coor[:,0]\n",
    "            c1y =coor[:,1]\n",
    "            polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[2],c1y[2]),(c1x[0],c1y[0]))) #for goal space\n",
    "            # polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[5],c1y[5]),(c1x[4],c1y[4]),(c1x[2],c1y[2]),(c1x[0],c1y[0]))) #for stimulus space\n",
    "            temp1.append(4*np.pi*polygon.area/(polygon.length)**2)  \n",
    "        temp.append(temp1)\n",
    "    dimensions.append(np.array(temp))\n",
    "real_data = np.array(dimensions[0])-np.array(dimensions[1])\n",
    "\n",
    "#plotting\n",
    "fig, ax =plt.subplots(1,1,figsize=(6,3.))\n",
    "data_dif=real_data\n",
    "plt.plot(f.times[w_start[:-1]+20], data_dif.mean(0), c='orange', label='difference')\n",
    "plt.fill_between(f.times[w_start[:-1]+20],data_dif.mean(0)+ data_dif.std(0)/np.sqrt(data_dif.shape[0]),data_dif.mean(0)- data_dif.std(0)/np.sqrt(data_dif.shape[0]),alpha=0.2,color='orange')\n",
    "plt.axhline(0,c='k',linewidth=0.8)\n",
    "for y in [0,0.4,1.7,2.3,3.8]:\n",
    "    plt.axvline(y, linestyle='--', color='k', linewidth=0.8)\n",
    "try:\n",
    "    #if plot significant period too, need to use the cluster_statistic() function to find time points first\n",
    "    plt.scatter(np.take(f.times[w_start[:-1]+20],coor), [-.05]*len(coor),c='r',marker='_')\n",
    "except:\n",
    "    pass\n",
    "ax.axvline(0,c='k')\n",
    "ax.axhline(0,c='k',linewidth=0.5)\n",
    "ax.axvspan(0.4,1.7, alpha=.15,color='gray')\n",
    "ax.axvspan(2.3,3.8, alpha=.15,color='gray')\n",
    "ax.set_ylabel('Circularity difference',fontsize=16)\n",
    "ax.set_xlabel('Time (s)',fontsize=15)\n",
    "ax.set_yticks([-0.1,0,0.1,0.2])\n",
    "ax.set_yticklabels([-0.1,0,0.1,0.2],fontsize=15, va='center')\n",
    "ax.set_xticks([0,0.4,1.7,2.3,3.8])\n",
    "ax.set_xticklabels(['0',0.4,1.7,2.3,3.8] ,fontsize=15, va='center')\n",
    "ax.tick_params(bottom=True)\n",
    "ax.xaxis.set_tick_params(pad=10)\n",
    "for y in [0.4,1.7,2.3,3.8]:\n",
    "    ax.axvline(y, linestyle='--', color='k', linewidth=0.8)\n",
    "sns.despine(fig=fig, right=True, top=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79f9e9-2c14-479a-a2ad-f71c918004ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 4A\n",
    "#Note that for response circularity, it is the same codes but need to recode conditions based on response values\n",
    "resampled_rdms=[]\n",
    "for t in range(5):\n",
    "    rdms_ss_v=[]   \n",
    "    for i, f in enumerate(epochs):\n",
    "        m = metadata[i]\n",
    "        m['trial_type'] = 0#recode trials into trial_types based on all features\n",
    "        x_train = f.copy().pick(picks_p).get_data()#select channels\n",
    "        for j,l in enumerate(list(product((1,2,3),(1,2,3)))): #test stimulus space\n",
    "            m.loc[(m['whichSize']==l[0])&(m['whichColor']==l[1]), 'trial_type'] = j\n",
    "        y_train = m['trial_type'].values #for plotting stimulus circularity\n",
    "        if t!=0:\n",
    "            x_train, y_train= resample(x_train, y_train, stratify=y_train)\n",
    "        n_trials, n_sensors, n_times= x_train.shape \n",
    "        eeg_data =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))\n",
    "        for k in range(np.unique(y_train).shape[0]):\n",
    "            eeg_data[k,:,:] = np.mean(x_train[y_train==k],axis=0)\n",
    "        w_start = np.where(np.isin(f.times, f.times[0::20])==True)[0]#if want to draw time course\n",
    "        rdms = []\n",
    "        for n in range(len(w_start[:-1])): \n",
    "            rdms.append(np.mean(eeg_data[:, :, w_start[n]:w_start[n+1]], axis=2)) \n",
    "        rdms_ss_v.append(rdms)\n",
    "    resampled_rdms.append(rdms_ss_v)\n",
    "\n",
    "#Create a null distribution specific to stimulus conditions (you need to manually change the code inside the function first)\n",
    "shuffled_rdm_ss= Parallel(n_jobs=25)(delayed(shuffle_trial_label)(picks_p) for i in range(5000))\n",
    "\n",
    "#calculate real individual circularity\n",
    "resampled_ci_stim=[]\n",
    "for t in range(5):\n",
    "    temp=[]\n",
    "    if t==0:\n",
    "        pca_stim_ind , scaler_stim_ind= [],[]\n",
    "    for ss, rdm_arr in enumerate(resampled_rdms[t]):\n",
    "        m= metadata[ss]\n",
    "        circularity=[]\n",
    "        temp1,temp2=[],[]\n",
    "        for i in range(len(w_start[:-1])):\n",
    "            scaler = StandardScaler()\n",
    "            embedding=PCA(n_components=2)\n",
    "            coor = embedding.fit_transform(scaler.fit_transform(np.delete(rdm_arr[i],[1,4,7],axis=0))) #Discard the middle 3 conditions\n",
    "            c1x =coor[:,0]\n",
    "            c1y =coor[:,1]\n",
    "            temp1.append(embedding);temp2.append(scaler)\n",
    "            polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[5],c1y[5]),(c1x[4],c1y[4]),(c1x[2],c1y[2]),(c1x[0],c1y[0]))) #note this order is critical and can change\n",
    "            circularity.append(4*np.pi*polygon.area/(polygon.length)**2)\n",
    "        if t==0:\n",
    "            pca_stim_ind.append(temp1);scaler_stim_ind.append(temp2)\n",
    "        temp.append(circularity)  \n",
    "    resampled_ci_stim.append(temp)\n",
    "individual_ci_stim=[]\n",
    "for t in range(len(w_start[:-1])):\n",
    "    individual_ci_stim.append(np.array(resampled_ci_stim).mean(0)[:,t])\n",
    "individual_ci_stim=np.array(individual_ci_stim).T\n",
    "\n",
    "shuffled_ci_resampled =[]\n",
    "for t in range(5):\n",
    "    shuffled_ci = Parallel(n_jobs=25)(delayed( calculate_null)(rdms) for rdms in np.array(shuffled_rdm_ss)[:,t])\n",
    "    shuffled_ci_resampled.append(shuffled_ci)\n",
    "shuffled_ci_stim = np.array(shuffled_ci_resampled).mean(0)\n",
    "\n",
    "#manual cluster level statistic\n",
    "cluster_perm_total=  Parallel(n_jobs=25)(delayed(cluster_statistic)() for i in range(5000))\n",
    "threshold_total = np.percentile(cluster_perm_total, 95, method='closest_observation')\n",
    "coor=[]\n",
    "for x in a:\n",
    "    if len(x)>threshold_total:\n",
    "        coor.extend(x) \n",
    "\n",
    "#Plotting\n",
    "fig, ax =plt.subplots(1,1,figsize=(6,3.))\n",
    "data = individual_ci_stim\n",
    "plt.plot(f.times[w_start[:-1]+20], data.mean(0), c='orange', label='difference')\n",
    "plt.fill_between(f.times[w_start[:-1]+20],data.mean(0)+ data.std(0)/np.sqrt(data.shape[0]),data.mean(0)- data.std(0)/np.sqrt(data.shape[0]),alpha=0.2,color='orange')\n",
    "for y in [0,0.4,1.7,2.3,3.8]:\n",
    "    plt.axvline(y, linestyle='--', color='k', linewidth=0.8)\n",
    "try:\n",
    "    plt.scatter(np.take(f.times[w_start[:-1]+20], coor), [0.15]*len(coor),c='r',marker='_')\n",
    "except:\n",
    "    pass\n",
    "ax.axvline(0,c='k')\n",
    "ax.axvspan(0.4,1.7, alpha=.15,color='gray')\n",
    "ax.axvspan(2.3,3.8, alpha=.15,color='gray')\n",
    "ax.set_ylim(0.12,0.25,auto=True)\n",
    "ax.set_ylabel('Individual circularity',fontsize=15)\n",
    "ax.set_xlabel('Time (s)',fontsize=15)\n",
    "ax.set_yticks([0.15,0.2,0.25])\n",
    "ax.set_yticklabels([0.15,0.2,0.25],fontsize=15, va='center')\n",
    "ax.set_xticks([0,0.4,1.7,2.3,3.8])\n",
    "ax.set_xticklabels(['0',0.4,1.7,2.3,3.8] ,fontsize=15, va='center')\n",
    "ax.tick_params(bottom=True)\n",
    "ax.xaxis.set_tick_params(pad=10)\n",
    "for y in [0.4,1.7,2.3,3.8]:\n",
    "    ax.axvline(y, linestyle='--', color='k', linewidth=0.8)\n",
    "sns.despine(fig=fig, right=True, top=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc347b-f0c2-4fc0-b945-4a16217fa0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 4B\n",
    "rdms_ss_v=[]   \n",
    "for i, f in enumerate(epochs):\n",
    "    m = metadata[i]\n",
    "    m['trial_type'] = 0\n",
    "    x_train = f.copy().pick(picks_p).get_data()#select channels\n",
    "    for j,l in enumerate(list(product((1,2,3),(1,2,3)))): #test stimulus space\n",
    "        m.loc[(m['whichSize']==l[0])&(m['whichColor']==l[1]), 'trial_type'] = j\n",
    "    y_train = m['trial_type'].values #for plotting stimulus circularity\n",
    "    n_trials, n_sensors, n_times= x_train.shape \n",
    "    eeg_data =np.zeros((np.unique(y_train).shape[0], n_sensors, n_times))# eeg_data 1st dim matches num of features in conceptual model\n",
    "    for k in range(np.unique(y_train).shape[0]):\n",
    "        eeg_data[k,:,:] = np.mean(x_train[y_train==k],axis=0)\n",
    "    w_start = np.where(np.isin(f.times, [-0.5,0,0.4,1.7,2.3,3.8,4.3])==True)[0]  #select the few time window indicates\n",
    "    rdms = []\n",
    "    for n in range(len(w_start[:-1])): \n",
    "        rdms.append(np.mean(eeg_data[:, :, w_start[n]:w_start[n+1]], axis=2)) #not using similarity matrix just use raw data\n",
    "    rdms_ss_v.append(rdms)\n",
    "\n",
    "#perform PCA\n",
    "dfs_mds=[]\n",
    "categorical_model = list(product((1,2,3),(1,2,3)))\n",
    "for i in range(len(w_start[:-1])):\n",
    "    embedding = PCA(n_components=3)\n",
    "    scaler = StandardScaler()\n",
    "    a = [x[i] for x in rdms_ss_v]\n",
    "    coor = embedding.fit_transform(scaler.fit_transform(np.delete(np.hstack(a), [1,4,7],axis=0))) #exclude the middle color values\n",
    "    df = pd.DataFrame(np.hstack((coor,np.delete(categorical_model, [1,4,7],axis=0))), columns=['PC1','PC2','PC3','size','color'])\n",
    "    df['time']=i\n",
    "    dfs_mds.append(df)\n",
    "dfs_mds = pd.concat(dfs_mds, ignore_index=True)\n",
    "\n",
    "#plotting\n",
    "fig, axs =plt.subplots(1,len(w_start[:-1]),figsize=(4*len(w_start[:-1])+2,4),sharex=True, sharey=True)\n",
    "axs=axs.ravel()\n",
    "palette=['g','r']\n",
    "titles=['Pre-cue','Goal cue','Delay 1','Sample','Delay 2','Response']\n",
    "dim1='PC1'; dim2='PC2'\n",
    "feature='color' #depends on which 3 middle values were excluded\n",
    "circularity_stim=[]\n",
    "for i in range(len(w_start[:-1])):\n",
    "    sns.scatterplot(x=dim1, y=dim2, data=dfs_mds[(dfs_mds.time==i)&(dfs_mds[feature]!=2)], size='size', hue='color',legend=False,sizes=[50,90,140],\n",
    "                    edgecolor='k',palette=palette, ax=axs[i], zorder=2)\n",
    "    c1x, c1y =[],[]\n",
    "    for j,x in enumerate([0,2]):\n",
    "        c1x.extend(dfs_mds[(dfs_mds.time==i)&(dfs_mds[feature]==x+1)][dim1].values)\n",
    "        c1y.extend(dfs_mds[(dfs_mds.time==i)&(dfs_mds[feature]==x+1)][dim2].values)\n",
    "    axs[i].plot([c1x[0],c1x[1],c1x[2],c1x[5],c1x[4],c1x[3],c1x[0]],[c1y[0],c1y[1],c1y[2],c1y[5],c1y[4],c1y[3],c1y[0]], 'gray',lw=1.4,zorder=1)\n",
    "    axs[i].set_title(titles[i],fontsize=22)\n",
    "    axs[i].set_xlabel('PC1',fontsize=16)\n",
    "    axs[i].set_ylabel('PC2',fontsize=16)\n",
    "    axs[i].set(xticklabels=[],yticklabels=[])\n",
    "    axs[i].tick_params(bottom=False, left=False)\n",
    "    polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[2],c1y[2]),(c1x[5],c1y[5]),(c1x[4],c1y[4]),(c1x[3],c1y[3]),(c1x[0],c1y[0])))\n",
    "    circularity_stim.append(4*np.pi*polygon.area/(polygon.length)**2)\n",
    "    c = 4*np.pi*polygon.area/(polygon.length)**2\n",
    "sns.despine(fig=fig, right=True, top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89661d-6fcd-4a8b-95f9-7ce49d05893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 5A\n",
    "#Load coherence data (weighted phase lag index ) and run statistics\n",
    "con_file=[]\n",
    "for subj in range(22):\n",
    "    con_file.append(glob.glob('wpli_sub-%s.npy'%(subj+1))[0])\n",
    "con_data = []\n",
    "for con in con_file:\n",
    "    con_data.append(np.load(con,allow_pickle=True))\n",
    "#apply baseline\n",
    "tfr = mne.time_frequency.EpochsTFR(reconst_epochs_p3.info, np.array(con_data), times, freqs)\n",
    "tfr = tfr.apply_baseline(baseline=(-0.5,-0.3),mode='mean')\n",
    "#Pick only coherence to posterior channels\n",
    "X = np.transpose(tfr.copy().pick(picks_p).data.mean(1), (0,2,1)) #should be observations × time × frequencies × space\n",
    "#run the cluster based permutation analysis\n",
    "T_obs, clusters, p_values, _ = spatio_temporal_cluster_1samp_test(X, n_permutations=5000,threshold=1.96, tail=1,\n",
    "                            n_jobs=1, buffer_size=2000,adjacency=None, max_step=1, verbose=1, out_type='indices')\n",
    "\n",
    "#Plotting\n",
    "times = tfr.times\n",
    "# Create new stats image with only significant clusters\n",
    "T_obs = tfr.copy().average().pick(picks_p).data.mean(0).T\n",
    "T_obs_plot = np.nan * np.ones_like(T_obs)\n",
    "for c, p_val in zip(clusters, p_values):\n",
    "    if p_val <= 0.05:\n",
    "        T_obs_plot[c] = T_obs[c]\n",
    "        \n",
    "vmax = np.max(np.abs(T_obs))\n",
    "vmin = -vmax\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,3.5))\n",
    "plt.imshow(T_obs.T, cmap=plt.cm.gray,\n",
    "           extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    "           aspect='auto', origin='lower', vmin=vmin, vmax=vmax)\n",
    "plt.imshow(T_obs_plot.T, cmap=plt.cm.RdBu_r,\n",
    "           extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    "           aspect='auto', origin='lower', vmin=vmin, vmax=vmax)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.locator_params(nbins=5)\n",
    "ax.set_yticks(freqs[0::2])\n",
    "plt.axvline(0, linestyle='-', color='k', linewidth=1)\n",
    "for y in [0.4,1.7,2.3,3.8]:\n",
    "    plt.axvline(y, linestyle='--', color='k', linewidth=.8)\n",
    "plt.ylabel('Frequency (Hz)', fontsize=15)\n",
    "ax.set_xticks([0.2,1,2,3.1,4.1])\n",
    "plt.title('Frontomedial theta to posterior coherence', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565389d-0749-47f8-9c69-b378f482983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 5B and C\n",
    "delay1_clu= clusters[np.where(p_values<0.05)[0][0]]#tuple(time indices x freqs indices)\n",
    "#select delay time points: tfr.times==0 for 1st cluster, tfr.times==1.7 for second cluster\n",
    "time_mask = delay1_clu[0]>= np.where(tfr.times==0.)[0] \n",
    "\n",
    "delay1_clu_x = delay1_clu[0][time_mask]\n",
    "delay1_clu_y = delay1_clu[1][time_mask]\n",
    "freq_mask = np.logical_and(delay1_clu_y>=np.where(tfr.freqs>=4)[0][0], \n",
    "                            delay1_clu_y<=np.where(tfr.freqs<7)[0][-1]) #select 1st theta freqs cluster\n",
    "delay1_clu_x = delay1_clu_x[freq_mask]\n",
    "delay1_clu_y = delay1_clu_y[freq_mask]\n",
    "coh_ss = [x[(delay1_clu_x,delay1_clu_y)].mean() for x in X]\n",
    "#print correlation between coherence strength and behavior\n",
    "# for t in range(6):\n",
    "#     circularity = dfs_mds_ss.loc[dfs_mds_ss.time==t].groupby(['subject'])['ci_%s%s'%(0,1)].mean().values \n",
    "#     print(pg.corr(circularity, coh_ss, method='spearman',alternative='greater'),'BF10', \\\n",
    "#           pg.corr(rankdata(circularity), rankdata(coh_ss), method='pearson',alternative='greater')['BF10'].values[0])\n",
    "\n",
    "#Plotting for goal circularity\n",
    "#Here you can select the task epoch to plot by passing the index for time\n",
    "circularity =  dfs_mds_ss.loc[dfs_mds_ss.time==4].groupby(['subject'])['ci_%s%s'%(0,1)].mean()\n",
    "fig, ax=plt.subplots(1,1,figsize=(5.5,4))\n",
    "sns.regplot( coh_ss, circularity)\n",
    "ax.set_ylabel('Circularity', fontsize=16)\n",
    "ax.set_xlabel('FMT-to-posterior coherence', fontsize=16)\n",
    "sns.despine(fig=fig,top=True, right=True)\n",
    "ax.tick_params(bottom=False, left=True)\n",
    "ax.set_xticks([-0.1,0,0.1,0.2])\n",
    "ax.set_xticklabels([-0.1,0,0.1,0.2],fontsize=15)\n",
    "ax.set_yticks([0,0.2,0.4,0.6])\n",
    "ax.set_yticklabels([0,0.2,0.4,0.6],fontsize=15)\n",
    "\n",
    "#stimulus circularity\n",
    "delay1_clu= clusters[np.where(p_values<0.05)[0][0]]#tuple(time indices x freqs indices)\n",
    "time_mask = delay1_clu[0]>= np.where(tfr.times==0.0)[0] #select delay time points\n",
    "delay1_clu_x = delay1_clu[0][time_mask]\n",
    "delay1_clu_y = delay1_clu[1][time_mask]\n",
    "freq_mask = np.logical_and(delay1_clu_y>=np.where(tfr.freqs>=4)[0][0], \n",
    "                            delay1_clu_y<=np.where(tfr.freqs<7)[0][-1]) #select 1st theta freqs cluster\n",
    "delay1_clu_x = delay1_clu_x[freq_mask]\n",
    "delay1_clu_y = delay1_clu_y[freq_mask]\n",
    "coh_ss = [x[(delay1_clu_x,delay1_clu_y)].mean() for x in X]\n",
    "for t in range(6):\n",
    "    circularity = dfs_mds_ss_stim.loc[dfs_mds_ss_stim.time==t].groupby(['subject'])['circular_index'].mean().values\n",
    "    print(pg.corr(circularity, coh_ss, method='spearman',alternative='less'), 'BF10', \\\n",
    "          pg.corr(rankdata(circularity), rankdata(coh_ss), method='pearson',alternative='less')['BF10'].values[0])\n",
    "print('\\n')\n",
    "for t in range(6):\n",
    "    circularity = dfs_mds_ss_target.loc[dfs_mds_ss_target.time==t].groupby(['subject'])['circular_index'].mean().values\n",
    "    print(pg.corr(circularity, coh_ss, method='spearman',alternative='greater'), 'BF10', \\\n",
    "          pg.corr(rankdata(circularity), rankdata(coh_ss), method='pearson',alternative='greater')['BF10'].values[0])\n",
    "\n",
    "#Plotting scatter plot for stimulus circularity\n",
    "circularity = dfs_mds_ss_stim.loc[dfs_mds_ss_stim.time==4].groupby(['subject'])['circular_index'].mean().values\n",
    "fig, ax=plt.subplots(1,1,figsize=(5.5,4))\n",
    "sns.regplot( coh_ss, circularity)\n",
    "ax.set_ylabel('Circularity', fontsize=16)\n",
    "ax.set_xticks([-0.1,0,0.1,0.2])\n",
    "ax.set_xticklabels([-0.1,0,0.1,0.2],fontsize=15)\n",
    "ax.set_yticks([0,0.2,0.4,0.6])\n",
    "ax.set_yticklabels([0,0.2,0.4,0.6],fontsize=15)\n",
    "ax.tick_params(bottom=False, left=True)\n",
    "ax.set_xlabel('FMT-to-posterior coherence', fontsize=16)\n",
    "sns.despine(fig=fig,top=True, right=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7c665-3db4-475f-bcc2-bd7f08d78682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 5D\n",
    "delay1_clu= clusters[np.where(p_values<0.05)[0][0]]#tuple(time indices x freqs indices)\n",
    "#select delay time points: tfr.times==0 for 1st cluster, tfr.times==1.7 for second cluster\n",
    "time_mask = delay1_clu[0]>= np.where(tfr.times==1.7)[0] \n",
    "delay1_clu_x = delay1_clu[0][time_mask]\n",
    "delay1_clu_y = delay1_clu[1][time_mask]\n",
    "freq_mask = np.logical_and(delay1_clu_y>=np.where(tfr.freqs>=4)[0][0], \n",
    "                            delay1_clu_y<=np.where(tfr.freqs<7)[0][-1]) #select 1st theta freqs cluster\n",
    "delay1_clu_x = delay1_clu_x[freq_mask]\n",
    "delay1_clu_y = delay1_clu_y[freq_mask]\n",
    "coh_ss = [x[(delay1_clu_x,delay1_clu_y)].mean() for x in X]\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,3))\n",
    "clist=plt.cm.tab20([1,0])\n",
    "#dfs_mds_ss is the individual correlation dataframe containing performance and circularity index\n",
    "df = pd.DataFrame(np.concatenate([dfs_mds_ss.loc[dfs_mds_ss.time==0].groupby(['subject'])['error_total'].mean().values[np.where(coh_ss<np.percentile(coh_ss,50))[0]],\n",
    "                                  dfs_mds_ss.loc[dfs_mds_ss.time==0].groupby(['subject'])['error_total'].mean().values[np.where(coh_ss>np.percentile(coh_ss,50))[0]]]),\n",
    "                columns=['error'])\n",
    "df['group']=np.repeat(['Low coherence','High coherence'],11)\n",
    "df=pd.melt(df,id_vars='group',value_name='Response error')\n",
    "sns.boxplot(data=df,x='group',y='Response error',width=0.4,palette=clist)\n",
    "sns.despine(top=True,right=True)\n",
    "_=ax.set_xticklabels(['Low coherence','High coherence'], fontsize=16,rotation=0)\n",
    "ax.set_yticks([0.5,1,1.5])\n",
    "ax.set_yticklabels([0.5,1,1.5],fontsize=15)\n",
    "ax.set_ylabel('Response error',fontsize=16,wrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d4962-de6d-45c5-ae42-fd786c40c899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1daa3bfa-959d-4cfa-a6cd-1b6ca5f3900b",
   "metadata": {},
   "source": [
    "### FMRI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87877fbb-5e97-41ae-8937-b645da6fa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, masking\n",
    "from nilearn.image import mean_img, load_img, concat_imgs,index_img,binarize_img,math_img,clean_img, resample_to_img,smooth_img, threshold_img\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut, StratifiedKFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from itertools import product\n",
    "import itertools\n",
    "from joblib import Parallel, delayed\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.maskers import MultiNiftiLabelsMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ef95d-1d93-49b8-84ef-b71d8a2d0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_null(delay):\n",
    "    inds = np.random.choice(50, len(subj_list))\n",
    "    fnames = [glob.glob('s0%s_searchlight_goal_%s_smooth_shuffled_%d.nii.gz'%(sub, tr_, ind))[0] for sub, ind in zip(np.arange(1,22), inds)]\n",
    "    data = mean_img(fnames).get_fdata()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623231f7-c568-47a8-a2be-2516485921d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 6 \n",
    "#To run searchlight analysis for circularity index\n",
    "!python script_searchlight_circularity.py\n",
    "#To generate null distribution for circularity searchlight, note this takes very long time\n",
    "!python script_searchlight_circularity_null.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52156f6-c6eb-4460-9af6-c3d8f7de7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From each iteration, draw an shuffled image per subject and calculate mean_img; repeat 10000 times\n",
    "#The null data would be 10000 x n_voxel\n",
    "#At each voxel calculate p values by comparing to the null distribution of mean\n",
    "temp1 =  Parallel(n_jobs=30, prefer='processes')(delayed(create_null)('delay1') for i in range(10000))\n",
    "temp2 =  Parallel(n_jobs=30, prefer='processes')(delayed(create_null)('delay2') for i in range(10000))\n",
    "null_data=[]\n",
    "null_data.append(np.array(temp1).reshape((len(temp1),-1)))\n",
    "null_data.append(np.array(temp2).reshape((len(temp2),-1)))\n",
    "\n",
    "p_imgs=[]\n",
    "ref_img = \"s01_delay1_smooth.nii.gz\"\n",
    "for j, tr_ in enumerate(['delay1','delay2']):\n",
    "    data = mean_img([glob.glob('s0%s_searchlight_goal_%s_smooth.nii.gz'%(sub, tr_))[0] for sub in np.arange(1,22)]).get_fdata()\n",
    "    p_array=np.zeros(data.flatten().shape)\n",
    "    for i,v in enumerate(np.nonzero(data.flatten())[0]): #assign 1-pvalue to nonzero data (to save time)\n",
    "        null = null_data[j][:,v] #get the null distribution at vth voxel\n",
    "        p_array[v] = np.where(null <data.flatten()[v])[0].shape[0]/len(null)\n",
    "    p_imgs.append(new_img_like(ref_img, p_array.reshape(data.shape)))\n",
    "\n",
    "#Save the pvalue images somewhere, use software such as AFNI to threshold and visualize the result\n",
    "p_imgs[0].to_filename('.nii.gz')\n",
    "p_imgs[1].to_filename('.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede02ad-396a-4d25-b823-e2ec91017e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 6 group-level PCA visualization\n",
    "categorical_model = [1,2,3,4]\n",
    "scaler = StandardScaler()\n",
    "ROI_mask = '' #This is a mask corresponding to a significant cluster from searchlight analysis\n",
    "#Extract activity from the cluster\n",
    "cluster_activity=[]\n",
    "for sub in range(1,22):\n",
    "    cluster_activity.append(masking.apply_mask('s0%s_%s_smooth.nii.gz'%(sub, tr_), ROI_mask, ensure_finite=True))\n",
    "\n",
    "Xtrain=[]\n",
    "for sub, BOLD in zip(np.arange(1,22), np.array(cluster_activity,dtype=object)):\n",
    "    beha_fname = glob.glob('s0{a}_behavioural.csv'.format(a=sub))\n",
    "    behavioural = pd.read_csv(beha_fname[0], delimiter=',',encoding_errors='ignore')\n",
    "    #run-wise standardization\n",
    "    fmri_masked =np.zeros(BOLD.shape)\n",
    "    for sess in np.unique(behavioural['block_loop.thisN'].values):\n",
    "        fmri_masked[behavioural['block_loop.thisN'].values==sess] = scaler.fit_transform(BOLD[behavioural['block_loop.thisN'].values==sess])\n",
    "    y_train = behavioural['cue'].values\n",
    "    x_train = np.zeros((np.unique(y_train).shape[0], BOLD.shape[1]))\n",
    "    #prepare data matrix for PCA\n",
    "    for i in range(np.unique(y_train).shape[0]):\n",
    "        x_train[i,:] = np.mean(fmri_masked[y_train==i+1],axis=0)\n",
    "    Xtrain.append(x_train)\n",
    "\n",
    "#concatenate all subjects so that the resulting matrix is n_stimulus x (n_voxel*n_subj)\n",
    "embedding2 = PCA(n_components=3)\n",
    "coor = embedding2.fit_transform(standardscaler.fit_transform(np.hstack(Xtrain)))\n",
    "dfs_mds = pd.DataFrame(coor, columns=['PC1','PC2','PC3'])\n",
    "dfs_mds['cue'] = categorical_model \n",
    "sns.set_style('white')\n",
    "\n",
    "palettes = ['yellow','green','r','darkblue']\n",
    "circularity_xy=[]\n",
    "dim1='PC1'; dim2='PC2'\n",
    "#plotting\n",
    "fig, ax =plt.subplots(1,1)\n",
    "data=dfs_mds.groupby(['cue']).mean().reset_index()\n",
    "c1x =data[dim1].values\n",
    "c1y =data[dim2].values\n",
    "ax.plot([c1x[0],c1x[1],c1x[3],c1x[2],c1x[0]],[c1y[0],c1y[1],c1y[3],c1y[2],c1y[0]],lw=1.4, c='gray', zorder=1)\n",
    "sns.scatterplot(x=dim1, y=dim2, data=data, hue= 'cue',legend=False, ax=ax, palette=palettes, s=110,edgecolor='black',zorder=2, )\n",
    "ax.set_xlabel('PC1',fontsize=16)\n",
    "ax.set_ylabel('PC2',fontsize=16)\n",
    "ax.set(xticklabels=[],yticklabels=[])\n",
    "ax.tick_params(bottom=False, left=False)\n",
    "polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[2],c1y[2]),(c1x[0],c1y[0])))\n",
    "circularity_xy.append(4*np.pi*polygon.area/(polygon.length)**2)\n",
    "c = 4*np.pi*polygon.area/(polygon.length)**2\n",
    "ax.text(-30,10,r'$C = %.2f$'%c,fontsize=16)\n",
    "sns.despine(fig=fig, right=True, top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d28fd-dcbc-4016-8a5d-9ad3452dac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 7\n",
    "# grab center coordinates for atlas labels\n",
    "#This file is a mask derived from combining selected clusters from multiple searchlight analyses (goal delay 1&2, stimulus and response)\n",
    "coordinates = plotting.find_parcellation_cut_coords(labels_img='searchlight_ROI_combined.nii.gz') \n",
    "coordinates=np.delete(coordinates,[6,9],0)#we exclude the two ROIs with poor 2D geometry\n",
    "\n",
    "masker = MultiNiftiLabelsMasker(\n",
    "    labels_img='searchlight_ROI_combined.nii.gz',\n",
    "    standardize=\"zscore\",\n",
    "    memory=\"nilearn_cache\",\n",
    "    detrend=False,\n",
    "    n_jobs=20,\n",
    ")\n",
    "\n",
    "#Get time series of each cluster and calculate pairwise functional connectivity\n",
    "time_series=[]\n",
    "correlation_matrices=  []\n",
    "mean_correlation_matrix=[]\n",
    "for i in range(2):\n",
    "    fname=[]\n",
    "    for j,sub in enumerate(np.arange(1,22)):\n",
    "        img= 's0%s_delay%d_smooth.nii.gz'%(sub, i+1)\n",
    "        beha_fname = glob.glob('s0%s_behavioural.csv'%sub)\n",
    "        behavioural = pd.read_csv(beha_fname[0], delimiter=',',encoding_errors='ignore')\n",
    "        fname.append(clean_img(img, runs=behavioural['block_loop.thisN'].values, standardize=True, detrend=True))\n",
    "    time_series.append(masker.fit_transform(fname))\n",
    "    connectome_measure = ConnectivityMeasure(kind=\"correlation\",standardize=False)\n",
    "    # calculate correlation matrices across subjects\n",
    "    correlation_matrices.append(connectome_measure.fit_transform(time_series[i]))\n",
    "    # Mean correlation matrix across subjects can be grabbed like this,\n",
    "    # using connectome measure object\n",
    "    mean_correlation_matrix.append(connectome_measure.mean_)    \n",
    "\n",
    "\n",
    "#7A\n",
    "fig,ax = plt.subplots(2,1,figsize=(10,4), gridspec_kw={'height_ratios':[8,1]})\n",
    "plotting.plot_connectome(\n",
    "    np.zeros(data.shape),#Only plot the coordinates not connections so pass a zero array\n",
    "    coordinates,\n",
    "    node_size=30,\n",
    "    node_color=['gold']*3+['blue']*3+['green']*2+['red']*2, #pass a customized color scheme (goal delay1, goal delay2, stimulus and response)\n",
    "    edge_threshold=0.01,\n",
    "    edge_vmax=0.4,\n",
    "    colorbar=False,\n",
    "    axes=ax[0],\n",
    "    edge_cmap ='RdBu_r',\n",
    "    annotate=True,\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "#7B\n",
    "sns.set_theme()\n",
    "sns.set_style('white')\n",
    "fig, axs = plt.subplots(1,3, figsize=(11,5), gridspec_kw={'width_ratios':[5,5,0.3]})\n",
    "axs=axs.ravel()\n",
    "labels= ['iPCS','left OFC','IFS','MTG','olPFC','mPFC','POS','ITG','left V4','right V4']\n",
    "mask = np.triu(np.ones(10),1)\n",
    "data = mean_correlation_matrix[0]\n",
    "data=np.delete(data,[6,9],0) #we exclude the two ROIs with poor 2D geometry\n",
    "data=np.delete(data,[6,9],1)\n",
    "g1=sns.heatmap(data, xticklabels=labels, yticklabels=labels, annot=False,annot_kws={\"size\": 8},\\\n",
    "                    square=True, mask=mask,vmin=-0.3, vmax=0.3,center=0, cmap='RdBu_r',ax=axs[0],linewidths=0.5,cbar=False)\n",
    "data = mean_correlation_matrix[1]\n",
    "data=np.delete(data,[6,9],0) #we exclude the two ROIs with poor 2D geometry\n",
    "data=np.delete(data,[6,9],1)\n",
    "g2=sns.heatmap(data, xticklabels=labels, yticklabels='', annot=False,annot_kws={\"size\": 8}, \\\n",
    "                    square=True, mask=mask, vmin=-0.3,vmax=0.3,center=0,cmap='RdBu_r',ax=axs[1], linewidths=0.5,cbar_ax=axs[2])\n",
    "g1.set_xticklabels(labels, rotation=25)\n",
    "g2.set_xticklabels(labels, rotation=25)\n",
    "sns.set(font_scale=1.2)             \n",
    "# plt.show()\n",
    "\n",
    "#run statistics and multiple comparison correction\n",
    "cutoff=3\n",
    "for i in range(2):\n",
    "    ps=[]\n",
    "    for x,y in itertools.combinations(np.arange(len(roi_names_sl)),2):\n",
    "        if ~((x<cutoff and y<cutoff) or (x>=cutoff and y>=cutoff)):#don't include connections within clusters from the same delay periods\n",
    "            matrix = correlation_matrices[i][:,x,y]\n",
    "            result = pg.ttest(np.arctanh(matrix), 0)\n",
    "            ps.append(result['p-val'])\n",
    "\n",
    "    reject, ps_cor = pg.multicomp(ps,method = 'fdr_bh')#multiple comparison correction\n",
    "\n",
    "    j=0\n",
    "    for x,y in itertools.combinations(np.arange(len(roi_names_sl)),2):\n",
    "        if ~((x<cutoff and y<cutoff) or (x>=cutoff and y>=cutoff)):\n",
    "            if ~reject[j][0]:#set non significant correlation to zero\n",
    "                mean_correlation_matrix[i][x,y] = 0\n",
    "                mean_correlation_matrix[i][y,x] = 0 \n",
    "            j+=1\n",
    "\n",
    "    for x,y in itertools.combinations(np.arange(len(roi_names_sl)),2):\n",
    "        if (x<cutoff and y<cutoff) or (x>=cutoff and y>=cutoff):\n",
    "            mean_correlation_matrix[i][x,y] = 0\n",
    "            mean_correlation_matrix[i][y,x] = 0\n",
    "            \n",
    "#7C-F\n",
    "#Prepare the circularity and behavioural data\n",
    "scaler = MinMaxScaler()\n",
    "standardscaler=StandardScaler()\n",
    "dfs_mds_ss =[]\n",
    "for idx, (sub, xtrain) in enumerate(zip(np.arange(1,22), Xtrain)):\n",
    "    beha_fname = glob.glob('s0%s_behavioural.csv'%sub)\n",
    "    behavioural = pd.read_csv(beha_fname[0], delimiter=',',encoding_errors='ignore')\n",
    "    behavioural ['error_color'] =  (behavioural.answer_color-behavioural.correct_color) #no within subject scaling, do it across subjects\n",
    "    behavioural ['error_size'] = (behavioural.answer_size-behavioural.correct_size)\n",
    "    behavioural ['error_size_per'] =behavioural.error_size/behavioural ['size'] #size is a percentile construt\n",
    "    embedding =  PCA(n_components=2)\n",
    "    coor = embedding.fit_transform(standardscaler.fit_transform(xtrain))\n",
    "    df = pd.DataFrame(np.hstack((coor,categorical_model.reshape(-1,1))), columns=[0,1,'cue'])\n",
    "    df['subject']=idx\n",
    "    df['error_color'] = behavioural.error_color.abs().mean()\n",
    "    df['error_size'] = behavioural.error_size.abs().mean()\n",
    "    df['error_size_per'] = behavioural.error_size_per.abs().mean()\n",
    "    dfs_mds_ss.append(df)\n",
    "\n",
    "dfs_mds_ss = pd.concat(dfs_mds_ss, ignore_index=True) \n",
    "#scale the error terms across subjects before adding up for total error\n",
    "dfs_mds_ss['error_color'] =  scaler.fit_transform(dfs_mds_ss.error_color.values.reshape(-1,1))\n",
    "dfs_mds_ss['error_size_per'] =  scaler.fit_transform(dfs_mds_ss.error_size_per.values.reshape(-1,1))\n",
    "dfs_mds_ss['error_total'] = dfs_mds_ss['error_color']+dfs_mds_ss['error_size_per']\n",
    "\n",
    "for idx in range(21) :\n",
    "    c1x =dfs_mds_ss[(dfs_mds_ss.subject==idx)][0].values\n",
    "    c1y =dfs_mds_ss[(dfs_mds_ss.subject==idx)][1].values\n",
    "    polygon = Polygon(((c1x[0],c1y[0]),(c1x[1],c1y[1]),(c1x[3],c1y[3]),(c1x[2],c1y[2]),(c1x[0],c1y[0])))\n",
    "    dfs_mds_ss.loc[(dfs_mds_ss.subject==idx), 'ci'] = 4*np.pi*polygon.area/(polygon.length)**2\n",
    "\n",
    "#Plot scatterplot between connection and behavioural performance or circularity index\n",
    "#This is just an example for one of the scatterplots\n",
    "ind1,ind2=0,8 #Select which connection to plot\n",
    "delay=1 #select which delay\n",
    "sns.set_style('white')\n",
    "fig, ax=plt.subplots(1,1,figsize=(5.,4))\n",
    "sns.regplot(x=correlation_matrices[delay][:,ind1,ind2],y= dfs_mds_ss.groupby(['subject']).error_total.mean())\n",
    "ax.set_title('iPCS - ITG (Delay 2)', fontsize=17)\n",
    "ax.set_xlabel('Connectivity strength', fontsize=17)\n",
    "ax.set_ylabel('Response error', fontsize=17)\n",
    "sns.despine(fig=fig,top=True, right=True)\n",
    "ax.tick_params(bottom=True, left=True, labelsize=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
